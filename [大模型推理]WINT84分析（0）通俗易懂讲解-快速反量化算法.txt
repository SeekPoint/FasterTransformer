[大模型推理]🔥WINT8/4分析（0）: 通俗易懂讲解-快速反量化算法
https://zhuanlan.zhihu.com/p/657072856


DefTruth

目录
收起
0x00 前言
0x01 Weight Only Int8 论文走读
0x02 Weight Only Int8 原理简析
0x03 总结


0x00 前言
关键词：GEMM + Dequantize Fuse、Fast Int8ToFloat16、Weight Only Int8、PRMT、LOP3


还是那句老话：好记性不如烂笔头~工作中需要关注的新技术多了，自然很难把每个要点都记住。
有些就算当下记住了，过后难免还是会忘记一些细节。那就抽空动动笔，记录一些自己认为重要的技术细节。

这次要记录的是对Weight Only Int8/4的理解，算法出自 2022年的一篇MoE推理优化的论文：
Who Says Elephants Can’t Run: Bringing Large Scale MoE Models into Cloud Scale Production，作者来自Microsoft和NVIDIA。
在大模型推理的应用中，这项技术，大家应该是比较熟悉了。

NVIDIA亲自操刀，其主要的一个技术创新便是-Fast Int8ToFloat16（快速反量化，暂且这么称呼吧）。
先前也看到过一些大佬的解读。其中一些技术点和实现，看了也觉得非常有意思，比如反量化算法的设计、PRMT、LOP3等指令的应用等；
因此，这次，我也简单记录下个人理解。

提示：智商有限，尽量争取讲明白，但不保证个人理解是完全正确的。

0x01 Weight Only Int8 论文走读
本文主要关注论文中提到的Fast Int8ToFloat16算法，其他的将会一笔带过。

GEMM + Dequantize Fuse
kernel融合，基操勿六，将本来GEMM、Dequantize两个独立kernel融合成一个，
计算量不变，但是访存可以得到优化，可以缓解kernel执行中遇到的memory bound压力。
01.webp
并且在底层是调用了CUTLASS Grouped GEMM Kernel来完成Fuse后的kernel计算
（这个Grouped GEMM，不太懂，抽空再跑到CUTLASS挖一挖...）
02.webp
Weight Only Int8/int4
在模型量化这块，这篇论文使用的是Weight Only，也就是只量化权重。
对shape为[E,M,N]的权重，使用range-based的per-channel的对称量化
（也是就对M维度进行量化，如果是对N维度量化，则是per-token量化），得到shape为[E,1,N]的scale值，并保存下来。
在online运算的时候，再把int8值转换成FP16，和同为FP16的activations以及bias参与模型计算。
03.webp

总的来说，量化这块用的就是最普通的量化手段，但是只量化Expert（MoE中的专家模型）的权重，因为他们占了MoE>=90%的权重。
另外，论文也给出了一些关于 FP16 vs Int8/4的diff观察。
总的来说，就是weight only量化就是在做精度和性能/吞吐的折中吧。
可以看到，weight only只会带来微小的精度退化，因此，也不是很有必要再上QAT。
04.webp

Fast Int8ToFloat16算法
论文指出，他们在实验中发现，使用原生的native Int8ToFloat16做数值类型转换，性能并不符合预期。
于是，大佬们决定，
将native Int8ToFloat16（吞吐低，底层应该是走 PTX cvt指令->SASS，之后再抽空profile一下）替换成一系列高吞吐的ALU和FP16运算指令，
来巧妙地完成Int8->Float16的数值转换。这个算法，来源于两个观察：

【观察一】 对于任意 FP16 数 X，其中 1024 ≤ X <2048, 1024 将准确地表示在指数位，
而 int(X − 1024) 部分将直接存储在尾数中（二进制level原封不动地存储）。
例如，1027 的 FP16 表示（表示为0x6403) 将整数 3 直接存储在其表示的尾数位。
个人理解：0x6403中的尾部部分的二进制表示为0b0000000011(0x03)，这和整数3的int二进制表示完全一致。
05.webp

【观察二】对于任何整数 0 ≤ Y < 1024，我们可以构造 Y + 1024 的 FP16 表示，将指数设置为 1024 并存储 Y在 FP16 尾数中。
这很容易通过 0x6400 | Y 实现 ，因为 0x6400 是FP16 中 1024 的十六进制表示。
06.webp

看到这里，大家肯定很迷糊？这到底是个啥意思？先不要急，这两个观察，在下文都会详细解释。
大家只要记住，这个算法的本质是，使用一组性能更好的位操作及数学运算指令，
替代原来的低性能的数值转换指令，来等价地完成Int8ToFloat16。
我也尽量争取写得通俗易懂~

0x02 Weight Only Int8 原理简析
FP16数值介绍
在讲Fast Int8ToFloat16算法之前，需要先了解一下FP16的二进制表达，
以及FP16二进制转实数的逻辑，在这个基础上，再去理解Fast Int8ToFloat16算法就不难。
比较懒，网上有合适的图就直接拿过来用一下，侵删。
07.webp

我们可以看到，FP16的16位二进制，分别由符号为（1位）、指数位（5位）和尾数为（10位）组成。
这里，我们只关注，一般情况下，二进制转FP16浮点数实数，全0或全1，暂不考虑。转换公式为：
08.png

其中fraction表示尾数部分的10进制数值，公式中之所以是(fraction/1024)，是因为FP16尾数部分是10位，而2^10=1024。
我们先来看一个FP16二进制转实数的列子，比如0b0110010000000011这个FP16，其中组成为：

    0b 0 11001 0000000011
    # 符号位：0
    # 指数位：11001 -> 25 -> 25 - 15 = 10, 2^10=1024
    # 尾数部分：0000000011 -> fraction为3（尾数部分的10进制数值）

转换为实数表达为：

    1 * 2^10 * (1 + 3/1024) = 1027 = 1024 + 3 <-> 2^10 * 1 + 2^10 * 3/1024

这里，我们其实可以看到，FP16由二进制转实数的过程中，数值分别是有两部分进行表达的，
即，分别为，指数部分能表达的【2^10 * 1】以及尾数部分能表达的【2^10 * 3/1024】。这个点很重要，后续的分析会用到。
OK，FP16其实只要了解那么多就够了，我们接着来分析论文中Fast Int8ToFloat16算法提到的两个观察。

理解【观察一】
” 对于任意 FP16 数 X，其中 1024 ≤ X <2048, 1024 将准确地表示在指数位，
而 int(X − 1024) 部分将直接存储在尾数中（二进制level原封不动地存储）。
例如，1027 的 FP16 表示（表示为0x6403) 将整数 3 直接存储在其表示的尾数位。
“ 个人理解：0x6403中的尾部部分的二进制表示为0b0000000011(0x03)，这和整数3的int二进制表示完全一致。

首先，需要明确的是，观察一，是针对FP16数值而言的，它描述的其实是，FP16中落在[1024,2048)范围中的数值，在16个bit位上的分布规律。
要理解这个观察的含义，首先，我们要意识到，FP16的数值表达是存在【跨度】和【分段】的特性的（这两词是自己想的，为了方便表达）。
这到底啥意思呢？

首先，对于【分段】，很好理解就是上文所讲的，FP16由二进制转实数的过程中，数值分别是有两部分进行表达的，
即，分别为，指数部分能表达的，加上尾数部分能表达。

而对于【跨度】，则着重描述了指数位的数值表达特性，指的是，指数位部分表达的数值，总是以2^n作为一个跨度的。
比如指数位011000、011001、011010，分别代表的是512、1024和2048，他们在指数位二进制上的相邻的，但是在十进制上表达的实数是存在跨度的。
对于1024刚好是2^10，而指数位表达的下一个值则是2^11=2048，2^10和2^11的跨度是1024，
由于【跨度】的存在，(1024,2048）间的值无法再通过指数位表达，
只能通过尾数来表达（即尾数位表达剩余的(0,1024)，FP16实数=(-1)^Sx(1+.尾数)x2^10，此时S=0）。
而【恰巧】的是（往下看，会解释这个”恰巧“），对于2^10和2^11的跨度，它尾数部分，直接存储的就是int(x-1024)的二进制，方向为由低位至高位。
比如FP16数值1027(=1024+3)：

    0b 0 11001 0000000011
    # 符号为：0
    # 指数位：11001 -> 25 -> 25 - 15 = 10, 2^10=1024
    # 尾数部分：0000000011 -> fraction为3（尾数部分的10进制数值）-> 这里直接保存的就是 3 的int8对应的bit位

因此，扩展开来，对于[512,1024)间的FP16数值，也是如此，指数位跨度为2^9~2^10，
即512，512可以精确保存在指数位表达的数值，而(512,1024)则需要通过保存在尾数部分的(0,512)进行表达。

至于NV为啥选的是1024而不是512，我也不懂，但有一点个人理解。
就是FP16 的尾数为10位，能够精确表达的数值最大是1024，能够表达的最小单位刚好为1，即 (1/2^10)x2^10=1，
因此，【观察二】中指代的int数值，可以二进制level原封不动地保存在FP16的尾数部分。

Magic Number	指数部分值	    尾数表达范围	    尾数表达最小单位	    FP16实数
1024	        1024,
                0b11001=25,
                2^(25-15),
                2^10=1024	    [0,1024)	    (1/2^10)x2^10=1	    (-1)^Sx(1+.尾数)x2^10

2048	        2048, 2^11	    [0,2048)	    (1/2^10)x2^11=2	    (-1)^Sx(1+.尾数)x2^11

512	            512, 2^9	    [0, 512)	    (1/2^10)x2^9=0.5	(-1)^Sx(1+.尾数)x2^9

表中”尾数表达最小单位“列的(1/2^10)，对应二进制转FP16浮点数实数公式中的”1/1024“，.尾数=fraction/1024，
指数部分的值是会随着指数位的变化而变化的，这里是9~11。
我们可以看到，对于2048~4096, 10位二进制是无法精确表达[1,2048]，
此时FP16实数=(-1)^Sx(1+.尾数)x2^11，尾数部分能表达的最小单位为 (1/2 ^10)x2^11=2，
不是1，此时将0b0000000001保存在尾数，它表示的是2，而不是1。
因此，此时，不能直接将二进制level原封不动地保存在FP16的尾数部分。
同理512，最小表达是0.5，不是1，也不能直接将int，保存在尾数部分。
所以，这个Magic Number它必须就是1024。
举个列子吧，比如对于 512 + 3 = 515，FP16二进制的【错误表达】为：

    0b 0 11000 0000000011
    # 符号为：0
    # 指数位：11000 -> 24 -> 24 - 15 = 9, 2^9=512
    # 尾数部分：0000000011 -> fraction为3（尾数部分的10进制数值）直接保存3的二进制表达
    # 通过公式计算对应的实数
    1 * 2^9 * (1 + 3/1024) = 2^9 * 1 + 2^9 * 3/1024 = 512 + 3/2 = 513.5 【不等于 515】

理解【观察二】
“【观察二】对于任何整数 0 ≤ Y < 1024，我们可以构造 Y + 1024 的 FP16 表示，将指数设置为 1024 并存储 Y在 FP16 尾数中。
这很容易通过 0x6400 | Y 实现 ，因为 0x6400 是FP16 中 1024 的十六进制表示。”

基于观察一，对于[0,1024)的int数值（覆盖了uint8的表达范围），我们可以为其构造一个FP16表达，将int的二进制直接保存在尾数部分即可。
此时Y'=1024+Y，并且这这个构造可以通过按位或运算直接实现，即0x6400 | Y；
举个例子，对于int类型 Y=3，我们想要FP16的Y=3.0表达，可以先构造FP16的Y'=1024+3，将3的int二进制直接保存在Y'的尾数部分即可：

    # Y = 3 (INT 0x03)
    # Y'= 1024 + Y = 1024 + 3 (FP16)
    # (0x64 [高8位] | 0x03 [低8位] -> 扩展 -> 0x6400 | 0x0003 -> 0b01100100000000000 | 0b0000000000000011)
    # 反算得到FP16表达的Y: Y_FP16 = Y' - 1024 = 3.0, 此时得到的Y_FP16则是INT类型Y的数值等价的表达

可以看到，此时Y'的16位二进制，表达的是FP16数值1024+Y，那么为了获得FP16表达的Y，也很简单，就是用Y_FP16=Y'-1024 。
这看起来在做无用功，其实设计得非常巧妙！
因为这样就避开了使用吞吐很低的cvt指令做数值做转换，而是变成使用高吞吐的PRMT和SUB.F16 的位操作和数值运算指令。
我们直接看下NV的代码实现：
09.webp

可以看到代码中，通过内联汇编，调用了PRMT和SUB.F16进行反量化，而不是直接static_cast<>。
通过CUDA 12.2的文档，我们可以看到，数值转换指令的吞吐是比较低的。
10.webp
11.webp

而数值运算和位运算的指令吞吐则要高很多，并且代码中也使用“sub.f16x2” 指令同时计算一个pack 2个fp16的减法结果。
一个uint32按位保存了4个uint8的二进制，因此有2个“sub.f16x2”指令进行处理。
不过有个细节就是，代码实现中，并不是直接减去1024，而是减去1152（1024+128=1152），
这是由于量化存储的int值是加上了128的，因此在反量化时需要减去这个128。
12.webp

另外论文中提到，+128主要的作用是可以避免反量化时执行符号扩展的逻辑。
因此在反量化时，还要减去128，而在代码实现中，这步-128的操作，也直接和减去1024 Fuse成一个操作了，也算是一个小小的优化吧。
在FasterTransformer的源码中，获得量化int类型的FP16表达的具体实现为（Y_FP16=Y'-1024-128）：

    // Lastly, we subtract 1152 from our constructed number using fp16 math to get our signed integer as fp16.
    static constexpr uint32_t I8s_TO_F16s_MAGIC_NUM = 0x64806480;
    asm volatile("sub.f16x2 %0, %1, %2;\n" : "=r"(h[0]) : "r"(h[0]), "r"(I8s_TO_F16s_MAGIC_NUM));
    asm volatile("sub.f16x2 %0, %1, %2;\n" : "=r"(h[1]) : "r"(h[1]), "r"(I8s_TO_F16s_MAGIC_NUM));

而在0x6400 | Y的实现上，还使用了PRMT指令，这个指令的含义是permute，把两个输入操作数pack成一组8个字节，
并按照指定的mode打乱，获得新的32bit（4个字节），保存在目标寄存器上。

static constexpr uint32_t mask_for_elt_01     = 0x5250;
static constexpr uint32_t mask_for_elt_23     = 0x5351;
static constexpr uint32_t start_byte_for_fp16 = 0x64646464;
asm volatile("prmt.b32 %0,%1,%2,%3;\n" : "=r"(h[0]) : "r"(i8s), "n"(start_byte_for_fp16), "n"(mask_for_elt_01));
asm volatile("prmt.b32 %0,%1,%2,%3;\n" : "=r"(h[1]) : "r"(i8s), "n"(start_byte_for_fp16), "n"(mask_for_elt_23));

这里实际上做的就是 0x6400 | Y 的事情，只不是在实现上使用的两条 PRMT指令来完成，这个我会另开一篇文章来分析。

0x03 总结
本文分析了
Who Says Elephants Can’t Run: Bringing Large Scale MoE Models into Cloud Scale Production中
使用到的Weight Only Int8技术中快速反量化的技术原理。
论文中，也给出了该技术的效果，如下：
13.webp

可以看到，有不错的提速，量化模型的性能相比FP16提升到了1.28倍（由native Int8 I2F 1.05->1.28）。
本文到此就结束了，主要关注了如何理解Fast Int8ToFloat16算法基本原理，而关于代码讲解，以及PRMT、LOP3等指令的讲解，将会另开几篇文章来分析。

提问1：既然都要反量化到FP16，那么思考一个问题，不考虑显存占用，究竟是直接保存好反量化的FP16权重，
然后下次运算时直接加载进来进行运算快，还是online反量化再运算比较快？

回答1：个人理解，可能是后者。因为online反量化，使用这个快速反量化的算法，只需要高吞吐的2个指令；
而从global memory中加载保存的FP16，可能需要的latency会更高，按照NV PTX ISA 8.1 6.6章节-Operand Costs 中的文档说明，
从global memory中加载数据，需要 > 100时钟周期，而online反量化，是在寄存器操作的，非常快。
14.webp

更多大模型推理论文资料，见：

[大模型推理][20篇]🔥LLM推理论文集v0.2-300页PDF💡
62 赞同 · 8 评论文章

[大模型推理]🔥WINT8/4分析（1）：PRMT指令详解及FasterTransformer源码解析
12 赞同 · 1 评论文章

DefTruth：[大模型推理] WINT8/4分析（2）：快速反量化之INT8->BF16
8 赞同 · 0 评论文章

持续更新，错字先更后改...

编辑于 2023-09-30 12:28・IP 属地广东
[大模型推理]🔥WINT8/4分析（2）：快速反量化之INT8转BF16
DefTruth
DefTruth​
保持学习，永远少年。
已关注
8 人赞同了该文章
​
目录
收起
0x00 前言
0x01 BF16的特别处理
0x02 INT8转BF16源码分析
0x03 整体处理
0x04 总结
0x00 前言
关键词：快速反量化、__byte_perm、BF16、fp32_base

前2篇文章分析了NVIDIA在MoE大模型推理中用到的快速反量化技术的原理，以及INT8快速反量化到FP16的具体实现。
FP16和BF16都是大模型推理中常用的数据类型，由于BF16尾数只有7位，无法直接存储8位的int8数据，因此NV FasterTransformer中进行了特殊处理。
这篇继续讲一下INT8快速反量化到BF16具体是怎么处理的。另外，【强烈建议】先阅读完前2篇，再来阅读本篇内容。

[大模型推理]🔥WINT8/4分析（0）: 通俗易懂讲解-快速反量化算法
[大模型推理]🔥WINT8/4分析（1）：PRMT指令详解及FasterTransformer源码解析

0x01 BF16的特别处理
首先需要清楚BF16和FP16的区别，网上有图直接拿来用，侵删。
首先，我们可以看到，BF16的bit位构成为：符号位1位、指数位8位（和FP32指数位一样）、尾数位7位（小于FP16的10位）。
BF16的特点决定了我们不能直接使用FP16的方式进行处理。这是什么意思呢？
18.webp  图借，侵删

BF16尾数位只有7位，无法存储INT8，如何处理？
需要保存uint8的二进制bits，至少需要8位才可以，而BF16的尾数只有7位。因此不能直接将uint8直接保存在BF16的尾数部分。
那么怎么办呢？FasterTransformer的做法是使用FP32作为桥梁。我们知道FP32的尾数部分有23位，完全可以容纳uint8类型的8个bits。
而且，FP32 转换成 BF16 非常便捷，因为 FP32 和 BF16 的指数位数一样，都是8位，而尾数部分 FP32 的位数（23位尾数）大于BF16（7位尾数）；
因此FP32 和 BF16 的指数位表达的动态范围是一样的。FP32 转 BF16 只需要对FP32进行截断，保留高16位即可。

    UINT8 -> FP32_Expr = FP32_Base_Magic_Num bits | UINT8 bits
          -> FP32_Expr_Ori = FP32_Expr - (FP32_Base_Magic_Num + 128)
          -> BF16_Expr_Ori = [Truncate FP32_Expr_Ori High 16 bits]

0x02 INT8转BF16源码分析

关于交织的量化权重在内存中的布局、PRMT指令的解析（BF16转换中用到的__byte_perm是PRMT的CUDA Math API版本，功能一致）、解交织等知识，
在本文不再重复，请读者自行阅读：
DefTruth：[大模型推理] WINT8/4分析（1）：PRMT指令详解及FasterTransformer源码解析；
源码在NV FasterTransformer中的FastInterleavedAndBiasedNumericArrayConverter结构体。

    template<>
    struct FastInterleavedAndBiasedNumericArrayConverter<bfloat16_t, uint8_t, 4> {
        using result_type = Array<bfloat16_t, 4>;
        using source_type = Array<uint8_t, 4>;

核心实现在convert函数上，和FP16的类似的是，INT8到BF16的反量化函数，也需要负责处理几个事情，
即：对交织保存的权重，反量化并同时解交织。
我们直接来看源码吧，这里提供这个函数的一个增加了个人详细注释的版本。

 CUTLASS_DEVICE
    static result_type convert(source_type const& source)
    {
.....

代码首先对CUDA_ARCH进行了判断，只有sm80及以上才会支持BF16；
然后使用0x4B000000作为FP32的Magic Number，这个数值的选取逻辑，和在FP16中的分析是一致的，
请参考：DefTruth：[大模型推理] WINT8/4分析（0）:通俗易懂讲解-快速反量化算法，
因为FP32的尾数是23位，所以这里使用0x4B000000：

    0x4B000000 -> 0b 0 10010110 0000...0000
    10010110   -> 150 -> 150 - 127 = 23 -> 2^23 = 8388608
    + 128      -> 8388608 + 128 = 8388736

如上分析，使用0x4B000000意味着指数部分代表的精确值为8388608，最后还要额外减去一个128（量化），
因此需要被减去的数值为：8388608 + 128 = 8388736。
首先，我们看到，
源码使用了__byte_perm函数来构造一组FP32表达的数值（__byte_perm是PRMT的CUDA Math API版本，功能一致）：

    // {b, a} = {{0x4B, 0x00, 0x00, 0x00},{e3, e1, e2, e0}} index 7 -> 0
    uint32_t* fp32_intermediates_casted = reinterpret_cast<uint32_t*>(fp32_intermediates);
    fp32_intermediates_casted[0]        = __byte_perm(i8s, fp32_base, 0x7650);  // 0x{4B0000}{e0}
    fp32_intermediates_casted[1]        = __byte_perm(i8s, fp32_base, 0x7652);  // 0x{4B0000}{e1}
    fp32_intermediates_casted[2]        = __byte_perm(i8s, fp32_base, 0x7651);  // 0x{4B0000}{e2}
    fp32_intermediates_casted[3]        = __byte_perm(i8s, fp32_base, 0x7653);  // 0x{4B0000}{e3}

PRMT中用到的{b, a}，此时的排布为{{0x4B, 0x00, 0x00, 0x00},{e3, e1, e2, e0}}，
原理请参考：DefTruth：[大模型推理] WINT8/4分析（1）：PRMT指令详解及FasterTransformer源码解析，
这是由于权重先被交织保存的原因导致的。
在反量化时，要同时解交织，
即将{e3, e1, e2, e0}恢复成{e3, e2, e1, e0}。经过__byte_perm后，就获得了 Y_FP32 = 0x4B000000 | Y 的表达。
接下来，就是用Y_FP32减去8388736，获得原始int8值的FP32表达。
这里使用了CUTLASS_PRAGMA_UNROLL对四次的循环进行展开。

    CUTLASS_PRAGMA_UNROLL
    for (int ii = 0; ii < 4; ++ii) {
         fp32_intermediates[ii] -= 8388736.f; // f32 arr {e3, e2, e1, e0}
    }

最后，再次使用__byte_perm对FP32进行截断，这里实现成，同时对两个FP32，只取他们各自高位的2个字节，保留为BF16。
其中0x7632中的0x76表示选取fp32_intermediates_casted[2* ii +1]的高2字节，并将其保存在bf16_result_ptr[ii]的高2字节；
0x32表示选取fp32_intermediates_casted[2* ii +0]的高2字节，并将其保存在bf16_result_ptr[ii]的低2字节。
从而构成了一个32bit寄存器pack 2个BF16数据的形式。

    CUTLASS_PRAGMA_UNROLL
    for (int ii = 0; ii < 2; ++ii) {
        bf16_result_ptr[ii] =
         __byte_perm(fp32_intermediates_casted[2 * ii + 0], fp32_intermediates_casted[2 * ii + 1], 0x7632);
         // keep high 16 bits as BF16
    }

至此，就完成了4个UINT8转BF16的过程。

0x03 整体处理
上边讲的是4个uint8的反量化到BF16操作，我们最后再简单看下整体上是怎么处理所有uint8量化权重的反量化的。

    template<int N>
    struct FastInterleavedAndBiasedNumericArrayConverter<bfloat16_t, uint8_t, N> {
        static constexpr int VEC_WIDTH = 4;
    .....

首先，定义了device函数convert，模板FastInterleavedAndBiasedNumericArrayConverter类中，模板参数N表示uint8元素的个数。
该函数首先是检查N是否是4的倍数，VEC_WIDTH=4，也就是支持N为4的倍数的情况。
这是由于，只实现一次性处理4个元素的特例化的Converter，即：

    FastInterleavedAndBiasedNumericArrayConverter<bfloat16_t, uint8_t, 4>;

然后，在convert函数内部，实例化了这个特例化的Converter为：convert_vector_；
最后，通过for循环来每4个uint8元素一组进行处理，并且使用了CUTLASS_PRAGMA_UNROLL进行了循环展开。

    CUTLASS_PRAGMA_UNROLL
    for (int i = 0; i < N / VEC_WIDTH; ++i) {
       result_ptr[i] = convert_vector_(source_ptr[i]);
    }

至此，所有的uint8权重就被处理完了。最后，关于整体的调用链路，在上一篇有写，这里不重复了，
请参考：DefTruth：[大模型推理] WINT8/4分析（1）：PRMT指令详解及FasterTransformer源码解析

0x04 总结
本文整理了详细讲解了UINT8到BF16的快速反量化的具体实现，并且对比了BF16和FP16在处理上的区别。
但是这里目前为止只讲到了int8权重的反量化，int4权重反量化以及详细的调用流程还没提到，这将会再额外开几篇文章分析。

附大模型WINT8/4分析系列：

[大模型推理]🔥WINT8/4分析（0）: 通俗易懂讲解-快速反量化算法
6 赞同 · 0 评论文章

[大模型推理]🔥WINT8/4分析（1）：PRMT指令详解及FasterTransformer源码解析
12 赞同 · 1 评论文章

[大模型推理][20篇]🔥LLM推理论文集v0.2-300页PDF💡
62 赞同 · 8 评论文章

持续更新，错字先更后改......

编辑于 2023-10-01 11:27・IP 属地广东